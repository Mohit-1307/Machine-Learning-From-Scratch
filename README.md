# Machine Learning From Scratch ğŸš€

This repository is a comprehensive collection of **Machine Learning algorithms implemented step-by-step in Python**, covering **supervised learning, unsupervised learning, reinforcement learning, deep learning, dimensionality reduction, and model selection techniques**.

The goal of this project is **learning by implementation** â€” understanding how each algorithm works, how data is preprocessed, and how models are evaluated.

---

## ğŸ“‚ Project Structure

Each algorithm is implemented in a **separate Jupyter Notebook** for clarity and modular learning.

Machine-Learning-From-Scratch/
â”‚
â”œâ”€â”€ Data Preprocessing
â”‚   â””â”€â”€ data_preprocessing_tools.ipynb
â”‚
â”œâ”€â”€ Regression
â”‚   â”œâ”€â”€ simple_linear_regression.ipynb
â”‚   â”œâ”€â”€ multiple_linear_regression.ipynb
â”‚   â”œâ”€â”€ polynomial_regression.ipynb
â”‚   â”œâ”€â”€ support_vector_regression.ipynb
â”‚   â”œâ”€â”€ decision_tree_regression.ipynb
â”‚   â””â”€â”€ random_forest_regression.ipynb
â”‚
â”œâ”€â”€ Regression Model Selection
â”‚   â”œâ”€â”€ Accuracy_multiple_linear_regression.ipynb
â”‚   â”œâ”€â”€ Accuracy_polynomial_regression.ipynb
â”‚   â”œâ”€â”€ Accuracy_support_vector_regression.ipynb
â”‚   â”œâ”€â”€ Accuracy_decision_tree_regression.ipynb
â”‚   â””â”€â”€ Accuracy_random_forest_regression.ipynb
â”‚
â”œâ”€â”€ Classification
â”‚   â”œâ”€â”€ logistic_regression.ipynb
â”‚   â”œâ”€â”€ k_nearest_neighbors.ipynb
â”‚   â”œâ”€â”€ support_vector_machine.ipynb
â”‚   â”œâ”€â”€ kernel_svm.ipynb
â”‚   â”œâ”€â”€ naive_bayes.ipynb
â”‚   â”œâ”€â”€ decision_tree_classification.ipynb
â”‚   â””â”€â”€ random_forest_classification.ipynb
â”‚
â”œâ”€â”€ Classification Model Selection
â”‚   â”œâ”€â”€ Accuracy_logistic_regression.ipynb
â”‚   â”œâ”€â”€ Accuracy_k_nearest_neighbors.ipynb
â”‚   â”œâ”€â”€ Accuracy_support_vector_machine.ipynb
â”‚   â”œâ”€â”€ Accuracy_kernel_svm.ipynb
â”‚   â”œâ”€â”€ Accuracy_naive_bayes.ipynb
â”‚   â”œâ”€â”€ Accuracy_decision_tree_classification.ipynb
â”‚   â””â”€â”€ Accuracy_random_forest_classification.ipynb
â”‚
â”œâ”€â”€ Clustering
â”‚   â”œâ”€â”€ k_means_clustering.ipynb
â”‚   â””â”€â”€ hierarchical_clustering.ipynb
â”‚
â”œâ”€â”€ Association Rule Learning
â”‚   â”œâ”€â”€ apriori.ipynb
â”‚   â””â”€â”€ eclat.ipynb
â”‚
â”œâ”€â”€ Reinforcement Learning
â”‚   â”œâ”€â”€ upper_confidence_bound.ipynb
â”‚   â””â”€â”€ thompson_sampling.ipynb
â”‚
â”œâ”€â”€ Natural Language Processing
â”‚   â””â”€â”€ natural_language_processing.ipynb
â”‚
â”œâ”€â”€ Deep Learning
â”‚   â”œâ”€â”€ artificial_neural_network.ipynb
â”‚   â””â”€â”€ convolutional_neural_network.ipynb
â”‚
â”œâ”€â”€ Dimensionality Reduction
â”‚   â”œâ”€â”€ principal_component_analysis.ipynb
â”‚   â”œâ”€â”€ linear_discriminant_analysis.ipynb
â”‚   â””â”€â”€ kernel_pca.ipynb
â”‚
â”œâ”€â”€ Model Selection
â”‚   â”œâ”€â”€ k_fold_cross_validation.ipynb
â”‚   â””â”€â”€ grid_search.ipynb
â”‚
â”œâ”€â”€ Boosting
â”‚   â”œâ”€â”€ xg_boost.ipynb
â”‚   â””â”€â”€ catboost.ipynb

---

## ğŸ§  Topics Covered

### âœ” Data Preprocessing
- Handling missing values
- Encoding categorical data
- Feature scaling
- Train-test split

### âœ” Regression
- Linear & Polynomial Regression
- Support Vector Regression (SVR)
- Decision Tree & Random Forest Regression
- XGBoost & CatBoost

### âœ” Classification
- Logistic Regression
- K-Nearest Neighbors (KNN)
- Support Vector Machine (SVM)
- Kernel SVM
- Naive Bayes
- Decision Tree & Random Forest

### âœ” Unsupervised Learning
- K-Means Clustering
- Hierarchical Clustering

### âœ” Association Rule Learning
- Apriori Algorithm
- ECLAT Algorithm

### âœ” Reinforcement Learning
- Upper Confidence Bound (UCB)
- Thompson Sampling

### âœ” Natural Language Processing
- Text preprocessing
- Bag of Words model
- Sentiment classification

### âœ” Deep Learning
- Artificial Neural Networks (ANN)
- Convolutional Neural Networks (CNN)

### âœ” Dimensionality Reduction
- PCA
- LDA
- Kernel PCA

### âœ” Model Selection & Evaluation
- K-Fold Cross Validation
- Grid Search for Hyperparameter Tuning

---

## ğŸ› ï¸ Technologies Used

- Python
- NumPy
- Pandas
- Matplotlib / Seaborn
- Scikit-learn
- TensorFlow / Keras
- XGBoost
- CatBoost

---

## ğŸ¯ Purpose of This Repository

- Build **strong ML fundamentals**
- Understand algorithms beyond black-box usage
- Practice **clean ML pipelines**
- Serve as a **reference repo** for interviews & revision

---

## â–¶ How to Run

1. Clone the repository
   ```bash
   git clone https://github.com/Mohit-1307/Machine-Learning-From-Scratch.git
